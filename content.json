{"posts":[{"title":"打扫房间与第零篇文章以及开心","text":"游戏还是很好玩的，不过要节制经过了几天的思考，其实是拖延，终于决定写下这篇文章了。 metroid prime 通关了，可惜没有第一时间写在当时在游戏中进入冰雪世界的感受，估计再也不会写了。好消息是，我至少会几下，prime最后的boss战挺失望的，不是我想象的那种感觉。让鸟人族搞的仪式感那么强的大灾害只是一个大鱿鱼似的东西，真的是莫名其妙。而且，打完了也不会怎么样……我更喜欢超级银河战士的结局，生存恐惧的结局也很棒。对了，我看完了超银的全收集视频，这个游戏太超前了，在sfc时代营造出如此恐怖与寂静的感觉，好多细节。比如，再次和metroid宝宝相遇的那个场景，好多灰色的敌人一碰就碎，然后小蜜宝宝出现，把怪物吸收成粉末……再然后，只有一滴血的情况下，小蜜宝宝离开……如果当初我能在合适的时间相遇这部游戏该多好呢？不过，我已经和月下相遇了，在最合适的时间，真的也不能奢求太多。 读书比较顺利，运动与读书，继续加油最近恢复了运动与读书，看了spa相关的书籍。spa的开发也许更多是在实践中会有进步，如果只是理论，感觉不是太多。我看了测试驱动开发的书籍，有种恍然大悟的感觉。想起来接近10年前，我在学习mvc的时候，尝试所谓的测试驱动，可是失败了……那会儿如果我好好看看书，好好使用github也许我会更有收获也说不定。和老朋友交流的时候发现，我记了10年笔记，笔记软件各种更换，发现我没有利用起来这些记录……完全没有帮助，我没有再去看过，甚至自己也不想看。那么这些记录谁会看呢？我在这些记录上浪费的时间呢？当然有个好消息，我发现，我可以去看我曾经写下的博客，博客14年使用时间，最终文章寥寥可数，但是当我浏览的时候，确实找回了一些回忆，比如我学习symfony的记录。所以，我再次决定，从这篇开始换一个新的标题规则，所有文章用编号开始（这是为了便于后续文章的统计和管理），这边文章就是零了。 另外，我更换了新的域名，从长期的纠结中，我准备解放自己了。新的网址是https://oldyang.site，我发现我挺喜欢这个域名和名字的。这样就挺好。 最后，我要花一点点时间，至少把https://yangfs.blogspot.com上已经发表的文章迁移过来，作为一个新开始的纪念。可惜了那些最后没有发表的文章。 新的开始我和自己和解了，开始自由的做自己喜欢的和有价值的事情，用知识地图去规划未来的学习工作路径、做能让自己注意力集中的软件研究，写这个博客而不纠结于内容的错别字（我还是会定期修改的）。我有个观点，那些与众不同的时刻（可以称之为仪式感时刻）从事后看，大多平平无奇，而巨大的改变的时候在改变发生之时都是平常无比的。我希望未来回头看的时候，现在是一个绝大变化或者至少是个特别的时刻。","link":"/2023/04/22/0-clear-room-and-zero-and-happy-start/"},{"title":"建立一个全球可用的高速文件共享方案:开始就是曲折探索路径","text":"背景情况跨区域的团队协作存在的很多问题，想象一下，团队成员在伦敦、硅谷、西安、北京，团队中领域专家、有数据科学家等，提供IT工具协调大家共同开展一个项目存在很多挑战。我目前碰到的一个问题是，如何在这些团队成员间共享文件，核心需求是”快”，并且足够“简单”。作为简单来说，第一个能想到的就是FTP服务了，虽然古老，但是这个协议确实足够简单，然后就开始了我的折腾之旅。 FTP代理方案先考虑一个简单的情形，英国有个团队，北京有个团队，FTP服务器设置在北京团队的工作地点。这个情况下，北京的团队访问ftp速度是非常快的(8-10M/s),此时英国团队的访问速度就很慢了。当前阶段，现有团队喜欢了FileZilla做客户端下载文件。如何使得现有的使用习惯维持尽可能的不变提高速度呢？我想到了第一个方案就是FTP代理方案了。 FTP的代理不太多。第一个默认想到的就是反向代理方案，“对他使用Nginx吧”。但其代理缺点非常明显，就是配置特别复杂，因为FTP的passive模式，需要开放大量端口，这样公开报漏的服务器也不会安全[^1]。另外一个是使用专用的FTP代理软件，各种找只找到一个ftp.proxy[^2]。看了一下，12年没有更新了。当然，我还是花时间试了一下，结果失败了（纪念我的一个小时时间），并不是个很好使用的方案。 当然最重要的一点是，这样真的能提速吗？原来的模式是，英国团队直接访问北京的服务器，如果添加一个代理，比如代理放到香港，那么带宽如何处理？英国到香港再转到北京真的能速度快吗？这个时候我又思考了一下初衷，可能团队需要的就是一个“快”而“简”的文件共享服务。如果精力放到代理上是不是错过了关注点？ 一个新的独立的中间服务器建立一个中间FTP服务，地理位置上位于英国和中国中间，这样让两边都可以有适当的访问距离。这个时候我想到了点子是：掉换主服务器和备份服务器的位置。本来我的计划里，要用腾讯对象存储（COS）来备份所有的文件的，要不把方向调转过来，使用COS作为主服务器，然后利用本地每日进行备份，这样所有的团队都可以访问中间服务器，也许速度能平衡一下？说干就干～ 比较可惜，COS默认是不支持FTP服务的，官方提供了COSFTPserver工具[^3]。这里要吐槽一下腾讯云提供的这个工具，虽然不是不能用，实际配置过程中也确实有必要的提示，不过，使用腾讯自己的虚拟机依然配置困难（我碰到的问题是python依赖安装出问题，最终通过virtualenv模块顺利解决）。实际配置好之后，使用起来并不稳定，首先是挑选客户端，我常用的FE file explorer pro直接挂掉，以至于我一只以为没有配置好，最后发现FileZilla、cyberduck到是可以正常使用。缺点也是特别明显，就是速度慢，一些基本操作支持的并不好，比如，list速度比正常FTP慢很多，再比如，文件夹移动并不支持（勉强可以移动文件）。实现原理所限，估计也无法配置用户权限了。最后，也是最大的问题，速度的瓶颈在虚拟机器的带宽上。我之前没有想明白，就算用对象存储，实际流量也是先经过了架设FTP服务的虚拟机。比如，我使用轻量应用服务器的情况下，服务器只有1M带宽，然后此时的下载速度就只有可怜的200k/s。如果使用按量付费的服务器，带宽可以设置很高，此时北京的上传下载速度可以有1-2M/s，不过流量太贵了，1rmb/G，并且，如果按月选择高带宽的机器，价格更是贵的离谱，另外，可能还需要支付COS的费用。 最终这个方案暂时放弃了，价格贵，且很难保证稳定。我继续又想了，两个方案如果都不太行，是不是可以暂时放弃FTP的方案，关注于“高速文件共享”。 尝试围绕cloudFlare看看有没有合适的解决方案重新会到起点，我现在实际需要的是一个Building a Global High-Speed File Sharing System，这就让我想起了cloudflare，他的R2支持多地区部署，如果直接用R2是不是可以解决速度问题，并且，他的下载流量是不收费的，配合合适的工具[^4]，本地备份可以没有费用。cf试图解决的问题就是全球化部署应用，这种功能聚焦让我稍稍有点兴奋——爷我也要部署一个全球应用了。先找一些简单的项目试试，很可惜，cf的R2太新了，没有成型的适配项目（如果我有时间精力一定自己做一个……唉），我随手找到了flaredrive[^5]稍稍做尝试就配置成功了，作者自己也提供了在线试用[^6]。一开始我非常兴奋，但是稍加冷静发现了问题，过于简单了。比如，不支持重命名，不支持文件移动。我上传大文件的时候，没有任何进度提示，对用户过于不友好从而使这个项目不可用了。类似的我还找到了R2-Explorer[^7]，这次有了经验，看到TODO列表就知道不能用了,太简单了，甚至不能重命名文件夹。 简单的cf应用貌似走不通了，调研中我看到一个非常让我心动的商业软件R2FTP[^8]，如果把R2变成FTP会怎么样呢？是不是直接解决问题，可惜，这个网站貌似是个PPT产品，只有一个首页，github链接是空的，twitter上也没有人。不过，R2FTP到是给了我一个新的思路。还是围绕R2，能否有包装的比价好的外围工具呢？还真有！！sftpcloud[^9]貌似就是这样的工具，他提供服务端，然后数据存在R2里。尝试了，依然失败（哈哈，我都平静了，这是个围绕失败的故事）。问题原因不明，不过，大概成功了也不一定会有好的效果，因为毕竟数据中转还是在ftp服务所在的节点，当然也许还有继续尝试的价值，不过，这个方案只能暂时封存了。 幕间故事对于加速产品，也调研了一些商用产品，比如Ftrans[^10]、raysync[^11]，也尝试咨询了一下，不过这些方案都太有针对性了，基本都是关注于特定的领域（比如文件审核）。特别联系了一下ftrans的技术销售，具体实现方案需要在每个节点架设非常高性能的服务器，软件的授权价格也不是很美丽。 最终还是回到网盘方案对FTP方案进行了诸多尝试之后，最终，还是回到网盘方案，继续看看nextcloud吧。借助nextcloud我又重新熟悉了一下docker操作。其中部署也不是那么顺利，特别是我用的都是便宜虚拟机，部署还特别慢（我甚至在一台古老的台式机上部署了一个nextcloud）。部署过程中我发现了新的问题，nextcloud太重了，操作繁琐，真的是团队需要的工具吗？直观来看我的待办列表里一直有一项，“给nc配置s3后端”，总是无法完成。我就重新思考，nextcloud真的是我需要的吗？ 冷静下来，回到最开始的出发点，一个简单高速的文件分享服务。如果确定要用网盘实现，那么是不是还有其他的方案可以选择？于是，我不停的用关键词搜索其他的项目，然后得到了一串列表:seafile[^12],kodbox[^13],pydio[^14],filerun[^15]。seafile的简洁实用页面让我非常惊艳，真的是关注文件本身，而不是乱七八糟的其他东西（说的就是你，nextcloud）。kodbox可以找到serverless的部署教程，这也非常让我在意。在这个过程中，我不断的思考，“全球高速”，以至于要半夜从床上爬起来看CAP原理，想从最基础的角度考虑到我的解决方案，这是之前看过的一个重要博客出现在我脑海里————使用cloudflare部署全球高可用网站的方案[^16]。根据博客的思路，我完全可以把主机部署在就近的位置，用cloudflare的服务来就进回源，由于R2服务是最终一致的（也就是保证AP的），多节点主机也许也能获得不错的速度（至少保证能完成工作）。带着这个想法去看seafile的部署文件，发现了其中的问题：官方解答多节点部署必须相同域名，这个还不是主要问题，主要问题是官方的部署文件里使用memcache做小文件加速，必须每个节点同步，并且，官方不太保证这种部署方式，另外， 后端三个存储桶的保存要求，也让我意识到，目前这些网盘工具，关注点其实并不只是存储，重点在于meta信息（用户账户、文件评论、管理权限等）。 小节写了这么多，问题其实并没有被解决，不过，“我到底想要什么”这个问题在不断的失败、重新起步探索中被不停的聚焦了。比如，我越来越清楚，我需要的是个小而轻的工具。在多次部署cosftp的过程中，由于是用了COS存储，每次重新部署服务后，我之前的数据会突然出现在眼前，这让我意识到，我需要的是工具最好是和存储高度解耦的。一周的时间，每天1到数个小时，真的是让我重新找回了学习的感觉。 写下这段文字希望对后来人又帮助。 [^1]: 博客 Nginx反向代理FTP教程[^2]: 官网 ftp.proxy - FTP Proxy Server[^3]: 腾讯云文档中心 FTP Server 工具[^4]: 对象存储备份工具rclone Rclone syncs your files to cloud storage[^5]: flaredrive项目主页 github-flaredrive[^6]: flaredrive demo flaredrive demo试用[^7]: R2-Explorer项目主页 R2-Explorer[^8]: R2FTP主页 FTP Servers for Cloudflare R2 Storage[^9]: sftpcloud FTP &amp; SFTPas a service.[^10]: 飞驰云联高速ftp方案 Ftrans飞驰云联-飞驰传输[^11]: 镭速传输方案 镭速传输-专为企业提供大数据加速传输方案[^12]: seafile官网 Seafile开源的企业云盘[^13]: kodbox官网 kodcloud可道云[^14]: pydio官网 Pydio | Enterprise File Sharing &amp; Sync Platform[^15]: filerun官网 FileRun - Selfhosted File Sync and Share[^16]: 博客 分布式部署 cloudflared 让访客就近回源","link":"/2023/05/24/1-Building-a-Global-High-Speed-File-Sharing-System-Insights-and-Best-Practices/"},{"title":"FTP文件扫描脚本","text":"背景情况公司有个FTP，里面是一些共享文件：研发、行政等等都会往里面放东西。作为一个共享文件服务它足够简单，作为一个内网应用，也足够好用。用了很久了之后，现在要做一些改进，所以，希望知道目前一共有多少文件，这样也可以估算一下未来准备多少硬件。从安全角度考虑，我还没有拿到这个FTP系统的访问权限（拿到了我也不想贸然登录服务器，服务一切正常的时候尽量不要乱动）。统计文件的工作项目在我的“待办列表”里待了很久很久，终于在一个阳光明媚的上午，我准备把它处理好。 过程故事开始我把这个问题想简单了，我美滋滋的把这个任务丢给opencat(powered by gpt),结果，因为没有任何上下文的情况下，她给我回了一个shell脚本，用bash直接驱动lftp命令。 我隐约感觉不太对，直接的第一反应就是python。然后就开始了折腾，中间故事不表了，只说结果：我进坑了。ftplib是默认的库，运行效率特别低会卡死，考虑到有大量文件，默认的库可能确实性能不太好也可以理解（并不能……），我尝试更换了ftputil，结果中文支持不好。 最终回到了了bash+lftp的组合，值得庆幸的是，我很喜欢这个组合。并且也知道了，不要随便用python，ftp是那个年代的东西，就应该搭配同年代的工具 幕间休息 抓取数据和之前抓取数据不同，这次我并没有记录抓取进度，而是根据根目录下的文件夹，一个个抓取的。想起了上次抓取数据的经验，抓取过程肯定会断掉：所以断点重连和进度保存看似麻烦却实际上是必要的。这次让我惊喜的是，lftp自己有断点重连功能！！现在回头看看，不能太喜新厌旧。当然也有一些坑：比如文件名中可能有“空格”，这个被我考虑到了，但是问题出在存在“双空格”的情况，然后ls输出本来是多空格过滤为单空格:也就不能用awk了。改用用sed，然后发现macos的sed并不标准，准确的说，sed支持的正则表达式就不标准。文件名中的“&amp;”这些就更不用说了。再比如，日期字段并不规范，如果是去年及以前的文件，就会用年月日，而当年的文件就会显示具体时间。好在，这些小的细节gpt就可以帮忙了。 结果我拿到了所有的数据列表，截止6月5号的文件快照有了一份～可以作为后来分析扩容的基础了。更加开心的事情是，我的待办列表可以清除一项了。 最终代码123456789101112131415161718192021222324252627282930313233343536373839# this script (maybe) only working on mac# FTP server credentialsFTP_USERNAME=&quot;username&quot;FTP_PASSWORD=&quot;password&quot;FTP_SERVER=&quot;257.257.257.257&quot;#change this ^_^now=&quot;/firstdir&quot;OUTPUT=&quot;./output/now.txt&quot;#the functionfunction getFiles() { local path=&quot;$1&quot; if [[ ${path: -1} != &quot;/&quot; ]]; then path=&quot;${path}/&quot; fi local scriptpath=$(echo &quot;$path&quot; | sed 's/ /\\\\ /g' | sed 's/&amp;/\\\\&amp;/g') local output=$OUTPUT local remote_files local size line local filename local first lftp -u &quot;$FTP_USERNAME&quot;,&quot;$FTP_PASSWORD&quot; &quot;$FTP_SERVER&quot; &lt;&lt;EOF cd $scriptpath ls &gt; /tmp/files.txt # list all files and directories and save to a temporary file byeEOF while read line; do filename=$(echo &quot;$line&quot; | sed -n -r 's/^.*[Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec]{3} [0-9]{2}[[:space:]]+([0-9]{4}|[0-9]{2}:[0-9]{2}) (.*)$/\\2/p') echo $line &gt;&gt; &quot;${output}&quot; first=${line:0:1} if [[ $first == 'd' ]] ; then echo &quot;$path$filename&quot; getFiles &quot;$path$filename&quot; fi done &lt; /tmp/files.txt}getFiles &quot;$now&quot; 小结这个代码写完花了整整4个小时，一个上午的时间都在写，写的过程中想着还有其他的事情要处理。总之，并不是个很开心的工作，原因是心态问题，我一开始忘记了这个工作中肯定会出现各种“细节”需要不断调试，如果一开始就知道的话，心态能好很多。开心面对自己喜欢的工作应该是非常幸福的事情。","link":"/2023/06/05/2-find-how-many-files-on-ftp-server-with-script/"},{"title":"旅行游记","text":"去了大连。 感觉也是个很有意思的城市，长江路、天津街……这样的命名方式在青岛的时候也遇到过呢，作为2023年的行程，目前去过了大连、青岛、宁波。终于可以活动活动，感受中国的大好河山。 来到了十一假期，终于有空闲的时间可以思考了。想把博客的内容再更新一下，至少把旧博客都迁移过来，结果第一件事就是又又又更新了主题。这次更新希望能稳定一段时间。看看博客时间，高质量的输出依然很少，虽然最近做了不少东西，但是能静下心来写的依然很少。","link":"/2023/09/30/3-travel-of-dalian/"},{"title":"在ubuntu上部署一个php网站","text":"前言很久没有写教程了。其实日常写的东西并不少，但是我的博客还是断更了，距离上次写内容一个月以上了，这一个月的生活变化、人际关系变化特别多，总的来说：一切在客观上更加顺利了。写下这篇教程的时间，我有非常多的想法，但是要把想法落地落实，却又需要很多的时间和精力，本着尽量留下痕迹的方式，如果要写就尽量留下公开的痕迹吧，本着这样的思路写下这篇php网站的部署博客。 环境部署第一步是安装ubuntu，有一个可以部署应用的系统。由于本文撰写的时间，我选择了ubuntu 22.04(友人推荐的长期支持版)，至于ubuntu的安装这里不在赘述。 设置swap新的机器，最好设置一下swap防止内存不足，我的机器是2g，设置4g的交换空间应该就足够了。相关脚本如下： 12345678sudo swapon --show #查看是否设置了交换空间sudo fallocate -l 2G swapfile #创建交换文件# sudo dd if=/dev/zero of=/swapfile bs=1G count=2 #fallocate不可用时采用这个命令sudo chmod 600 swapfile #设置文件权限sudo mkswap swapfile #创建交换文件sudo swapon swapfile #启用交换文件echo &quot;/swapfile none swap sw 0 0&quot; | sudo tee -a /etc/fstab #添加内容到启动项中free -h #检查是否成功 我的系统是使用虚拟机安装的，一开始就被设置好了交换分区了。 安装docker第一步，肯定是安装docker，虽然整个网站是使用的php，不过，数据库最好还是容器化比较简单，安装docker按照教程来就可以。Install Docker Engine on Ubuntu,这是安装教程链接，使用过多次，每次都很顺利。 卸载旧版本：这个命令是卸载旧版本用的，如果没有安装旧版本，可以不用这个。 1for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done 设置docker的库,设置好docker软件库，为下载做准备 123456789101112sudo apt-get updatesudo apt-get install ca-certificates curl gnupgsudo install -m 0755 -d /etc/apt/keyringscurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpgsudo chmod a+r /etc/apt/keyrings/docker.gpg# Add the repository to Apt sources:echo \\ &quot;deb [arch=&quot;$(dpkg --print-architecture)&quot; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ &quot;$(. /etc/os-release &amp;&amp; echo &quot;$VERSION_CODENAME&quot;)&quot; stable&quot; | \\ sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/nullsudo apt-get update 安装docker,并进行测试安装效果 123sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-pluginsudo docker run hello-world 安装openoffice最好给openoffice单独建立一个目录，这样使用compose可以直接部署openoffice服务。使用docker compose来部署openoffice服务 123456789101112version: &quot;3&quot;services: onlyoffice: container_name: oo7 image: onlyoffice/documentserver:7.4.1.1 ports: - &quot;9000:80&quot; restart: always environment: - JWT_SECRET=&lt;yourkey&gt; 当然可以使用命令解决以上问题：！！！注意，yourkey需要修改！！！ 1234567891011121314151617mkdir -p ~/workspace/onlyofficecat &lt;&lt;EOF | tee ~/workspace/onlyoffice/docker-compose.yamlversion: &quot;3&quot;services: onlyoffice: container_name: oo7 image: onlyoffice/documentserver:7.4.1.1 ports: - &quot;9000:80&quot; restart: always environment: - JWT_SECRET=&lt;yourkey&gt;EOFsudo docker compose up -d #下载镜像，启动服务 如果想停止服务，可以使用一下命令 sudo docker compose down http://localhost:9000,可以通过这个网址查看是不是部署成功了。 onlyoffice的镜像可能很大，可以把镜像提前准备好来节省实际部署时候的时间. 123sudo docker images #这个命令查看镜像，找到要导出镜像的idsudo docker save 0fdf2b4c26d3 &gt; oo7.tar #导出镜像文件，其中参数就是镜像idsudo docker load &lt; oo7.tar #导入镜像 这里还有一个细节：下载好的镜像，可以放在一个公开的服务器上（比如放到对象存储上），这样用wget下载速度就快了。例子：wget https://www.example.com/directlink/bj/webs/oo7.tar 这里的细节其实很多，因为虚拟机是NAT模式联网，我通过web界面把文件上传到了服务器上。这时候发现如果我当初安装的server版，没有gui，那么这个工作还非常难以实现呢。虽然就算有GUI，也是因为我有自己的文件中转服务才能简单实现。 部署php环境然后就是部署php环境的阶段，虽然按照正常流程是php、mysql、nginx这样依次安装，但是这样效率太低了，这里还是推荐用docker安装： https://github.com/ogenes/docker-lnmp根据教程就可以部署php网站了。 鸣谢特别鸣谢我的妻子，她帮我写了一个小的手册文章，所以我有时间精力写下这篇教程。","link":"/2023/11/09/4-how-to-setup-up-a-php-website-with-mysql-on-a-new-ubuntu/"},{"title":"部署一个支持小组开展工作的jupyter环境","text":"前言python统一环境从来都是一个复杂的问题，特别是要多人合作的时候（甚至两个人合作要统一环境也经常出问题）。环境配置不一致很容易打击团队的积极性，让本来应该聚焦到具体问题解决上的注意力被分散。晚上有很多教程，都是只是讲某个技术细节的配置，很少有完整的方案。本教程详细介绍一下如何配置一个帮助管理员或者小组领导带领小队完成数据分析任务的jupyterhub环境，包括环境安装、用户配置等诸多细节。 内网穿透是的，从内网穿透开始讲，太多的教程都会忽略服务最终如何访问到的问题，好像所有人都有了很好的网络环境，实际上的情况往往很复杂，很多新手可能就被“卡”在这里。当然，本教程也忽略了操作系统的安装这个步骤，尽量做到取舍有度。 假设你已经有了一台可以上网的ubuntu主机，本教程是基于一台NAT模式的虚拟Ubuntu进行的。登录tailscale的官网，找到对应的安装命令。 curl -fsSL https://tailscale.com/install.sh | sh相关的详细教程可以在官网查到https://tailscale.com/download/linux,官网贴心的附带了分步骤安装教程。 其实只是需要下载一个25MB的包，因为网络的原因，这个步骤我失败了很多次。原因是tailscale的服务器在境外。如果能有一个安装了科学透明代理的网络环境，这类服务安装起来会简单很多，等后续的教程中我在补这部分吧。 启动的命令是： tailscale up 这里有个小提示：命令行给出的地址，并不需要一定要“本机”登录，实际上我用我的笔记本电脑访问了图上的地址，完成了验证。 下一步我在macbook上安装tailscale终端，就可以进入虚拟的内网，访问服务器了。跨区原因不能在应用商店安装，用brew安装也很方便。 123456brew install tailscale #安装tailscalesudo tailscaled install-system-daemon #启动为系统服务#sudo tailscaled uninstall-system-daemon #这个命令应该是注销系统服务tailscale uptailscale ip #查看ip地址tailscale status #查看设备情况 启动服务就可以把终端和服务器放在一个内网里了，这样可以随时访问到NAT中的虚拟服务器了。我的教程这里相当于一个例子，如果你使用的是不同设备，官网有详细的教程，链接在这里安装终端的链接 本教程重点在于全面细节，为了能够随时访问到该机器，自然还要在机器上安装sshd服务，命令如下： 12sudo apt install sshsudo systemctl ssh 然后，使用你自己习惯的ssh终端，通过tailscale服务的内网地址，就可以访问到服务器了。这里有个个人碰到的小提示：我在tailscale使用中，有时明明服务在线，但是却访问不到主机，此时我发现使用ipv6就可以轻松访问到主机了。 安装conda第一时间找到官方教程总是不会错的Miniconda，根据教程可以完成安装。 官方的教程如下： 1234mkdir -p ~/miniconda3wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.shbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3rm -rf ~/miniconda3/miniconda.sh 以下教程均是按照官方脚本执行之后配置。不过，这里强烈建议不要完全按照官方教程来做，原因见后面章节。如果你能把教程全部看完之后再操作，建议把minicoda安装在/opt/miniconda/。 安装好之后，记得运行~/miniconda3/bin/conda init bash这个命令把miniconda加入到环境变量里,然后重新登录一下就激活了conda的base环境了。 安装jupyterhub虽然也有非常方便的tljh安装方式，不过，实际使用的时候tljh特别考验网络速度，完全没有conda来的快捷，另外使用conda安装更好定制，所以，这里推荐用conda来安装jupyterhub。 123456789101112conda create -n python310 python=3.10 #创建基础环境conda create -n jupyterhub --clone python310 #根据基础环境创建jupyterhub环境#这里提供一些可能会用到的管理命令#conda env remove --name jupyterhub #适当的时候可以删除环境#conda env list #现实当前的环境列表conda activate jupyterhub #激活环境#conda search -c conda-forge nodejs --info #找到允许的版本列表conda install nodejs #安装nodenpm install -g configurable-http-proxy #这个可以安装代理服务python3 -m pip install jupyterhub jupyter notebook #这个可以顺利安装notebook#如果网速过于慢，可以试试国内的源#pip install jupyterhub jupyter notebook -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com 经过上面的操作，jupyterhub已经装好了，可以尝试启动jupyterhub了。 1234mkdir ~/jupyterhub #创建用来放置jupyterhub的目录cd ~/jupyterhub/jupyterhub --generate-config #会生成配置文件模版jupyterhub -f jupyterhub_config.py #运行jupyterhub,这个命令一般也用来测试jupyterhub文件 如果一切正常访问http://ip:8000这个地址就可以看到jupyterhub界面了，然后就是把jupyterhub配置的可用。这里特别提示一下，jupyterhub默认使用的是linux系统本身的用户账户系统，用户名和口令就是linux系统本身的用户名和口令，新建用户也是同理。 配置jupyterhub配置jupyterhub也是很重要的一环，基本而言就是围绕配置文件进行；不过在那之前可以考虑先把内核配置好。 12345pip3 install ipykernelpython3 -m ipykernel install --name=jupyterhub --display-name &quot;dev_default&quot;#jupyter kernelspec list #列出kernel#jupyter kernelspec uninstall jupyterhub #卸载kernelpip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas # 安装一个包看看虚拟环境中的包是否在kernel中生效 两个心得： 因为使用jupyterhub环境安装的jupyterhub（怎么读着那么绕……），所有jupyterhub再后来的子用户就默认都具有jupyterhub这个虚拟环境，安装kernel指少在这里并不是必须的（更绕了），但这里还是提一下，如果想给所有用户都配置可选的kernel意外的困难，毕竟jupyterhub就装在jupyterhub虚拟环境下（……）； pip -i参数可以加快安装速度，另外也不用额外的更改源，意外的好用。 12345``` pythonc.JupyterHub.admin_users = {'peter'}c.Spawner.default_url = 'tree'c.LocalAuthenticator.create_system_users = True 以上命令分别是设置默认界面、设置管理员、设置添加用户方式等等，这个方面可以参考文档，当然直接阅读jupyterhub --generate-config命令生成的模版学习如何配置也是非常棒的。学习的重点就是更改一些配置，然后用命令直接运行看看结果。很多问题是在使用中发现的，比如现在jupyterhub添加用户是会报错的。 这个报错是我意料中的，原因自然是权限不足，目前普通账户无法创建用户。（意料中出错，然后修正，有点测试驱动的意味在里面呢）。为了解决以上问题，同时也是为了方便日常管理jupyterhub服务，把这个注册为系统服务。 12345678910111213141516171819202122232425which jupyterhub #这个命令确定Environment的目录#这段脚本根据你自己的情况改一下再用sudo sh -c &quot;cat &lt;&lt;EOT &gt; /etc/systemd/system/jupyterhub.service[Unit]Description=JupyterHub[Service]User=rootEnvironment=&quot;PATH=/home/peter/miniconda3/envs/jupyterhub/bin/:/usr/local/bin/:/usr/bin/:/usr/sbin/&quot;ExecStart=/home/peter/miniconda3/envs/jupyterhub/bin/jupyterhub -f /home/peter/jupyterhub/jupyterhub_config.py[Install]WantedBy=multi-user.targetEOT&quot;#以上命令是以root用户安装conda为例子进行的sudo systemctl daemon-reloadsudo systemctl start jupyterhubsudo systemctl enable jupyterhubsudo systemctl status jupyterhub#sudo systemcl restart jupyterhub#sudo systemctl disable jupyterhub#sudo systemctl stop jupyterhub#sudo journalctl -u jupyterhub -n 50 #这是查看日志的命令 这样整个jupyterhub就配置好了。再次尝试添加用户，成功添加。然后用管理员登录，给这个用户设置一个密码sudo useradd -m -s /bin/bash hubuser &amp;&amp; sudo passwd hubuser，用户就可以登录了，然后我们就看到了错误。 这是说明一开始就配置错误了，conda被安装在了用户目录下，新添加的用户完全访问不到。等于是官方教程的mkdir -p ~/miniconda3这个命令开始就出错了，应该把miniconda安装在/opt里才比较合适，事到如今，只能尝试修复一下权限了。 12sudo -u hubuser /home/peter/miniconda3/envs/jupyterhub/bin/jupyterhub-singleuser -hchmod 755 ~/home/peter/ 提示：如果有机会重新配置conda，一定要安装在/opt目录。 虽然配置工作很辛苦，不过至少环境可以统一了，如果用户多一点（也不需要特别多，节约的精力依然可观）。 用户目录配置sudo mkdir share &amp;&amp; chmod 777 share/首先建立一个目录供所有人共享使用。 建立起基本的目录结构mkdir pub codes project 通过以下方式可讲目录的权限给到任意用户。 123456cd /var/share/project/mkdir 01-notebookssudo suchmod 777 01-notebookscd /home/hubuser/ln -s /var/share/project/01-notebooks 01-notebooks 通过以上设置，用户就可以 小结至此，一个小型的可多人共享的jupyterhub配置好了，团队需要共享的代码可以放到share目录中，权限根据linux系统的权限来管理。对于管理员来说，需要进行的操作无非就是如下几种： 创建用户，并设置密码以及忘记密码时重置密码 维护项目目录的文件内容 把目录挂载给指定用户，并设置适当的权限 删除用户的权限或者不允许其再看到对应的目录 给虚拟环境安装适当的软件包 以上就是管理员所有操作的枚举，只要管理员熟悉了以上操作，就可以让团队利用好jupyterhub顺利推进工作啦。","link":"/2023/11/10/5-jupyterhub-for-team-work/"},{"title":"typescript的类型在运行时就没有了","text":"最近开发碰到一点点坑typescript在运行时的类型已经不存在了。所以，虽然我在request.params中可以很好的推断获得number，但是当在prisma的where中进行调用的时候还是会出现类型错误。这个坑浪费了我1个小时。","link":"/2022/10/13/about-ts-type-not-in-runtime/"},{"title":"开始了我的kaggle学习","text":"学习的背景感谢我的妻子来到我的生活，她真的是非常有智慧的女性，我感觉治愈了有点点在低谷的我。在她的鼓励下，我重新开始看kaggle，居然顺利学完了所有的入门课程，现在开始关注一些比赛。我的工作推进虽然缓慢，不过我内心平静了很多，感觉可以慢慢推进做一些事情了，虽然现在，偶尔还是很困并且容易劳累，也许我该健身一下。又是个碎碎念的博客呢。生活的中的细节多了，我目前也能静下来了，和形而上学和解了，未来也许可以写出来不错的文章。放张图记录一下最近的生活吧～ 相关信息原文地址：开始了我的kaggle学习","link":"/2022/04/15/arc-0-start-my-kaggle/"},{"title":"如果更早遇见是不是会好一点呢？","text":"读完了《推理竞技场》。 跟自己说着很久没有看书了，然后抽了两个半午休看完了这本书。我自己对这本书评价一般，虽然读起来确实流畅，也多少有些海猫的影子，不过，时间不太对吧。 我为什么不去直接读《竹林中》呢？我也许应该再去看看干货更多的书籍，而不是把很多哲学思想再加工的半成品推理小说？ 看完了这本书，让我有点不太想打开下一本书，也就是收获没有那么多。 好多东西堆在我眼前，可以推进我的业余项目、可以把工作精进搞好、可以去看纸牌圣经练习手法、可以学习视频制作（对了安装了fcpx，可以有时间学习视频编辑了）、可以去把之前买的狗头人桌游打开、可以尝试去运动…… 貌似又有了那种感觉，我拥有那么多东西，可是却没有办法很好的享受这些，如果我能静下心来，明明可以创造出很多很多愉快的记忆和成果的。 于此同时，我的本职工作，其实内容很简单，上传下达，文件检查，感觉还是没有把状态管理好，目前做的这些工作有点消耗精力。 静下心来，静下心来，平静的心情来感受生活是我自己目前需要做的。 原文地址","link":"/2022/04/15/arc-1-book-arena/"},{"title":"侦探AI-把浪漫的图灵实验变得更加浪漫","text":"提前说明，有少量剧透～ 好久没有读早坂的作品，虽然上木系列好像通过5部完结了，不过我也没有找到资源，我在看完了一部专业书籍之后想调剂一下，因此选了这部小说。17年的作品，里面对AI的描写，很多是经不起推敲的，比如：ai如果能知道自己有框架问题，他自己怎么发现框架这个词接地呢。哈哈，经不起推敲的细节明摆着，也让我看下去了，这不就是我喜欢的作品吗？依然是我喜欢的风格，给出一个胡扯的案件，然后构建故事框架，让这个胡扯的案件变得合理，小心翼翼的透露信息，精心的构建故事，尽量让前后合理。同时又在细节上注入思考，在人物上尽量让其可爱。真是部不错的作品。所以，每天看视频也没有啥吸收和输入，我要不要再好好看看书呢？如果能让我静下来的话，看书不好吗？","link":"/2020/10/08/arc-2-AI-romantic/"},{"title":"学习symfony的流水日记-0","text":"开始新的crud学习，拒绝浮躁，静心修炼，争取喜欢现在的自己。特别说明：本篇依然是流水账日记。 早起给机器添加了内存，拆了隔壁电脑的两条内存，这样我就有8g内存可以用了，很开心说不上，资源是利用起来了。开始学习symfony的crud，争取今天做点什么东西出来。symfony的安装教程 开始顺着教程走。 1php composer.phar create-project symfony/website-skeleton AccessControl 先建立个项目看看，运行之后发现检查依赖没有通过，果然还是下载symfony的二级制版本吧。检查php依赖好好把php配置好。二进制的软件去github下载了，休息休息吧。 symfony check:requirements之后产生了报告，修改时区，安装依赖， 123apt-cache search php-domapt installphp --ini 检查配置文件修复时区问题。chongqing php不认识呢,修改php的timezone，让symfony完全通过吧。 使用命令行工具建立项目。时间还是超级长，原因就是symfony使用了自己自带了composer.phar这个是没有任何的代理加持的。还是应该把整个composer全局化，并配置镜像代理。 12sudo mv composer.phar /usr/local/bin/composercomposer config -g repo.packagist composer https://packagist.phpcomposer.com 全局的代理配置好，然后重新用symfony建立项目。配置好这些之后依然exit status 128，我怀疑，其实项目已经建立好了。毕竟这个是git的报错，看着报错信息好像是我没有指定正确的用户名和邮箱，这个放一下，继续后续开发学习。建立demo项目的命令失败了。git被拒绝，看来还是要把git配置好才行。git加速教程按照这个教程配置git。希望一条命令可以解决问题。如果不行还是要调试一下git，毕竟后续还要下载好多安装包。正确的设置方式可以参考这里这里。设置好之后，github速度快很多了。如果下次碰到问题，再重新设置一次。（clash的支持很有必要）继续安装demo项目，项目自带了sqlite3数据库，这样看着就好多了。重新安装php对sqlite3的支持库，然后继续更新。突发奇想，要不我就不用mysql了？目前没有看到sqlite3有什么不好，如果并发数不是那么多的话，如果没有事务要求的话……先看看吧，再犹豫一下比较好。status 中128的错误还是在报错，简单设置一下吧。 12git config --global user.email &quot;test@mail.com&quot;git config --global user.name &quot;MyName&quot; 设置好之后以后再报错就再说。刚刚就是上午的工作了，进行了午餐，之后继续下午的工作。一抬手就发现根本无法使用demo项目，并不了解程序的运行逻辑和入口。这个就放一边，先把入门的hello world写好再说。路由模块找不到对应的方法，也就是说，我的apache应该没有配置好或者模块哪里有问题。自带的symfony服务器工具非常好用和方便。 我发现一个问题，利用apache的子目录来进行测试，是因为我太喜欢80端口了。这个习惯不好。另外，symfony本身这个命令建立的网站就是独立网站了，最好能有一个独立虚拟目录给他，这样才是便于测试和理解的。 继续调整了配置后发现，在这个框架下，public才是网站应该指向的地址，这样逻辑就顺了，symfony new project_name 中的name原来是最顶级的名字，把他理解为自模块应该是我在mvc.net中学到的不好的习惯。 虽然现在访问起来依然是index.php/lucky/number。不过网站是可以访问了。根据开发建议使用symfony自带的服务器这一点，我觉得还是忽视这个问题比较好。解决问题的教程在这里 完成了create pages的教学，下一步看from模块，争取一个模块一个模块的看过去，完成symfony的学习和使用。 form模块就很复杂了，休息了一下，把form模块的代码实践完。label写好了，中文也能改，这不就是把我昨天做二维码界面的工作重构了吗？感觉还是挺好的。虽然，我记忆中，mvc耦合文件图片等等事情的时候，我还是没有怎么搞懂……至少是没有找到最佳实践。 看完了form模块，从数据库和模板里面我选择模板，哈哈，虽然每个都很麻烦，但是，数据库可以再等等嘛~……等等，我刚刚应该是调用task_success失败了，虽然解决了也大致知道怎么回事儿了。还是按照文档的顺序来看吧（打下这句话的时候，我已经看完了）。","link":"/2020/07/02/arc-3-symfony-0/"},{"title":"学习symfony的流水日记-1","text":"昨日和宇视科技的技术人员了解硬件信息因此断更，今天继续我的symfony流水账学习。昨天突然想到我的应用可能一开始就要处理1对多关系，想想有点头疼呀。 早起集中精力先看controler文档。看文档发现的心得，虽然我在用php，但是当我因为一个文本没有编辑好，结果整个项目无法访问时，当我看到type hint给contrler传入一个接口，结果这个接口直接可以用时（肯定用了依赖注入把）。我就想，我是不是只是用php语法去实现了某个东西，好多都是静态语言的特性。只是因为用了php就想php扮演的多大的角色（当然，php也很重要），肯定是不合适的。 流水笔记有个好处，就是当我想向别人输出的时候，重定向到这里，感觉能轻松很多，也有所收获和积累。 中国企业的求生欲，藏在微信里 有点累，读一下水文，放松放松。放松好了，集中宝贵的经历，看教程，毕竟，后续还想安排通读DEMO教程呢。 模板看完了。继续看配置模块。配置看完了，要看数据库访问了。 午餐时间，吃了点东西，然后继续配置我的开发环境，把最近的文档整理一下，另外，看看能不能给机器搞个双显卡。^_^，日常折腾呢。 服务器资源访问不到。这个感觉问题更大一点，虽然目前还不是太重要就是了。 中午折腾机器，发现安装独立显卡后，集成显卡没有输出了。这是硬件特性吗？暂时不管了，继续看数据库访问吧。今天周五，周六放假等着我呢。数据库也看完了。这样最基本常用的模块我就都看过了。虽然部署，配置等等也有很多模糊和需要学习的地方，bundle也没有看，ajax也没有看到，用户验证部分也没有看到……哈哈，总之，我先把demo模块读一次吧。 这次是读代码了，仔细想想，我没怎么好好看过代码，经常是自己摸索出奇奇怪怪的套路，这次好好看一下是个不错的机会。 ……打开代码，vs提示要我配置git和php……又开始折腾了，我是真的喜欢折腾静不下心来啊。看了一小下demo的代码，感觉……没有太大感觉，还是自己上手至少先把qr编辑器做好吧。 启动apache2，开始建立我的qr配置部分，然后慢慢的就把各个部分都做好。php bin/console make:controller qrConfigController生成控制器。 引入bootstrap部分我就迷茫了……哈哈，没有那么简单的事情呢。{{ encore_entry_link_tags('app') }}这个总是报错。 原来我终于到了需要先把css编译才能使用的这一步。安装yarn还安装错了。哈哈。Google一下我发现我遇到的是wsl的bug……暂时放弃webpack把，这个功能感觉过于先进了。curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -这个命令出错了。 下班回到家然后，工作上感觉还是在推进一下比较好，因为稍稍有点危机感。打开电脑，配置好代理，命令敲完，发现借助现代的工具配置项目果然还是简单呢。 123export https_proxy=http://127.0.0.1:6152;export http_proxy=http://127.0.0.1:6152;export all_proxy=socks5://127.0.0.1:6153 代理帮了大忙了。 建立一个新项目吧。crud还是要学习一下的。brew services start mysqlHomebrew 都能管理服务了……真是变化大啊放个9年前的帖子，不知道还能不能用，至少mysql -uroot可以直接访问数据库是真的。 可以用，直接建立好数据库了。php总是调用3.7版本，是环境变量的问题原来是zsh的问题……真是各种坑等着你啊。……仔细思考了一下，并不是这样的！！环境变量的顺序没有错，/usr/local/bin确实是在最前面，问题可能是我刚刚才安装php到local，而没用重启，我怀疑zsh缓存了每个环境变量下的目录，所以找不到我的PHP。重启一下terminal看看； 果然啊！！居然是和上一次一样的错误。就是安装php扩展后，命令行可以正确执行gd图像生成，可是apache2不行。","link":"/2020/07/04/arc-4-symfony-1/"},{"title":"科研文献审稿经验心得分享","text":"初衷在实验室学习的过程中，经常能从导师那里接到一些审稿文章，而后进行一些审稿工作。开始几次审稿的时候我自己都还没有发过文章，文献读的也少，因此，吃力程度可想而之。借助google在网上固然可以得到很多相关的资料，其中很多帮助很大，但是大部分资料都是审稿回复的一些基本语句，还有就是，学术大牛对在投文章如何看待审稿意见的建议。 我一直以为对于任何事物的学习过程而言，开始的时候总是有其特殊的困难，因此对于开始一项学习而言，需要最入门简单的教程。而对于审稿工作，每个研究生第一次接触的时候总会有这样那样的困惑或者是更一般的无从下手的迷茫。笔者自己面对新工作的时候经常会不知道干什么而拖到Deadline的最后一刻，利用Deadline生产力原则来完成工作，不过我知道，这当然是不好的，事物的学习还是遵从它自然的规则更好。因此，写这篇随笔来记录一下自己的审稿心得，一方面是对自己学习的总结，另一方面希望对初次进行审稿的研究生同学提供一点点帮助。 审稿的基本概念考虑到这是一篇基础性文章，请允许我不厌其烦的介绍审稿的诸多细节，虽然对于很多人特别是经历过科技文章写作投稿的人来说，这些都是无意义的堆砌，但是，我回忆自己学习经历的初期，确实对“审稿”，“期刊”，“编辑”这些基本概念也是要一点点的查过来的，因此我想对于初学者，这种基本的介绍还是有帮助的。 首先是出版社（比如 Elsevier, Wiley等都是著名的科技文献出版社），出版社是一个商业组织，出版社旗下有很多的期刊，这些期刊上面刊登着世界各地的研究者以科技论文的形式发表的自己的研究成果。不同的期刊关注科学研究的不同领域，而出版社负责组织这些期刊。可想而知，科学研究的内容是一般是复杂的，而且针对具体的领域又很有深度，管理好一个期刊使期刊可以真正促进对应的学科进步是一件困难的事情。另一方面，出版社也非常关心的一件事情是如何保证自己期刊文章的质量。而审稿制度就可以在一定程度上解决这个问题。 简单说明一下文章发表的流程，从文章产生的角度，首先最开始当然是研究者选择课题，进行研究，并将研究成果以科技论文的形式撰写出来并投稿到自己想要发表的期刊。然后期刊的编辑会接收到文章，编辑的工作就是初步检查文章的内容并确定这篇文章是否符合期刊的发表要求。由于文章多涉及专业的研究内容，因此，编辑在确定文章的研究课题大致符合期刊要求之后，需要进一步对研究质量进行检查。编辑会在这个文章对应的研究领域里选择若干名专家，将文章发送给这些专家，由专家对文章的研究价值、创新性、研究严谨性进行评审，并给出审稿意见，这些审稿意见会返回给编辑和作者，一方面帮助编辑判断该文章是否适合发表，另一方面帮助作者完善其研究。当一篇稿件送到研究者手中时就形成了审稿的工作。 审稿的意义说明了审稿的作用之后，来考虑一个问题，我们为什么要审稿呢？ 虽然这个问题看似高大上，有点近乎哲学的意味了。不过，其实还是很简单的问题。讨论为什么要审稿之前，我们来说一下“学术共同体”。这是一个稍微抽象的概念，大致上可以理解为所有从事科研工作的人员的集合。科学研究是一个漫长艰辛的过程，时间长，工程庞大，内容繁杂，稂莠不齐，同时还经常会反复，因此，科研工作非单一个人、组织、甚至是国家可以完成的。所以，科学研究者们互相帮助，有时获取资源，有时提供帮助，在长期的运行过程中，慢慢形成了一个有利于科学研究进步的组织结构和行为规则这就是学术共同体了。 对学术共同体有了了解之后，其实很多科研中遇到的概念都可以很好理解了。比如，为什么学术造假时非常恶劣的行为？那是因为造假的研究结果会在学术共同体内传播，影响很多人的研究，甚至将一个学科引向错误的方向毁掉一个学科。为什么研究成果一定要以发表论文的形式来表现？那是科学研究的成果只有发表出来，才能使学术共同体内的其他人了解到你的研究，从而帮助整个学术共同体推进研究的进程。 解释完上面的内容，审稿工作的意义也就很好理解了。从学术共同体的角度来说，审稿工作是每个科研工作者的义务。就像Google Scholar首页上那句话一样“Stand on the shoulders of giants”，可以很容易的想见，如果没有大量优质的文献支撑，一个研究者很难了解自己到研究领域的研究历史、研究重点，没有对当前的研究进展的了解作为基础的话，创新的研究成果就无从谈起了。因此，每个研究者需要利用自己所具有的对自己研究专业的知识、经验帮助其他科研工作者完善他们的研究工作，或者提出批评、或者提出建议。同时帮助编辑判断研究工作的价值，一定程度上防止有问题的文章发表出来。 是不是很有责任感？其实，对于审稿人而言，审阅他人的文章也是自己学习进步的过程。不同观点看待一个事物的过程中很容易产生创新的想法。 需要完成的工作既然本文想着重介绍的是入门，那么就来说说针对一篇审稿我们要做什么。 首先，拿到一篇审稿的第一步，了解审稿的期刊来源，期刊研究的是什么内容。同时了解文章作者的背景信息，包括这个作者的研究领域，所在的单位，之前的研究成果。这些都是帮助我们理解文章的基本内容。针对入门的同学，再补充一个更为基础的问题，哪里去找这些内容呢？如果是中文审稿的话用作者的名字一搜也许就找到了，如果是英文呢？其实道理是一样的，查找信息的方式就是把英文当成你的母语设置关键词去检索你需要的信息。很多研究生同学（包括我自己）都是开始的时候对英文很头疼，不过和其他所有的学习过程一样，只要坚持其实很快就能适应。这里说明一下了解任何一个作者的捷径，就是看作者发表的文章，这个相当于第一手资料，快速的浏览作者的文章的题目、摘要、图片你就可以了解作者。很多时候拿到的审稿和我们自己的课题稍微有点远，因此我自己的经验是先通过作者之前的研究了解基本背景，这样读的时候，更方便发现文章的研究重点而防止自己的精力被不重要的细节分散。 然后，仔细阅读文章全文。所谓仔细阅读全文的方法因人而异，有些人会逐段读，而我自己因为经常被人打断思路，而且我读英文比较慢，因此我会先和看一般文献一样快速通读，了解文章到骨架，然后细致阅读补充内容。审稿阅读的和一般阅读是不同的，因为你是有目的，所以需要“带着问题读”。这篇文章的研究背景是什么？创新点是什么？当前研究的瓶颈或者关注的焦点是什么？解决什么问题？使用了什么方法？作者得出了什么结论？作者如何用他的实验支撑结论？作者使用的方法是否可行？作者的数据是否可信？带着这些问题会帮助你更好理解文章，这中间也需要阅读相关的文章。 最后，阅读完之后就是要给出审稿意见，指出作者需要修改的错误，提出需要完善的地方，帮助作者改进他的文章。另一方面对于不合格的文章需要给出拒稿意见，要指出作者的问题。 对文章进行评价对于一篇文章，需要给出处理建议，一半包括小修(Minor revision)、大修(Major revision)、拒稿(Reject)，根据什么标准来判断给出处理建议呢？ 对于一篇科技论文来说，文章的基础应该是科学的严谨性，严谨性在于理论多选取、方法的可行性、甚至是数据的可信性。另一方面，学术论文的价值在于创新，文章的核心工作不应该是重复性的，应该是全新的，同时创新性来源于对前人研究的总结改进不能是凭空产生的，对于一篇合格的文章，引言部分需要核心论证的就是文章的创新性，因此，不合格的文章大致是以下三种情况： 1. 没有创新性； 2. 数据不可信； 3. 研究工作没有意义。 如何发现和说明这些问题呢？这就需要审稿人阅读相关领域的文献，动笔计算文章中的数据，认真的分析文章研究结论的合理性。我个人经验是阅读文章的过程中，一定要形成自己的观点，一旦你有了自己的观点和认识，你就不会被作者牵走。更重要的是，你的观点和作者不同的时候，你就就会有自己的问题，这些问题会帮助你分析和这个审稿相关的文献，加深你对作者研究内容的理解，最终使你可以更加客观评价审稿文献。 结尾以上就是我个人对于审稿工作的经验分享，希望对后来人有帮助。","link":"/2015/12/08/arc-5-science-paper-review/"},{"title":"阅读与js相关的书籍","text":"更新尽量不要停如果没有什么内容，我更新一下读书笔记吧，就是这两本。 js的可靠代码，最终内容还是设计模式。这样也是非常有道理的，可靠就是解除耦合，解除耦合就是合理的设计模式，这本书自然而然的讲了如何用js实现各种设计模式。让我惊艳的地方有两点： 全书围绕着一个例子展开讲，把一个大会支持网站的例子的每一个部分都渐进式的讲到了； 全书都在坚持采用“测试驱动开发”的模式，讲解了如何测试各类设计模式； 内容方面，丰富的设计模式，让我反思之前没有好好学习应用设计模式是不是一种小损失。另一本，全家桶就看的比较快速了，毕竟更新的内容太多了。官方甚至都在主教程里推荐next.js了，肯定很多内容不相符。不过，我还是认真阅读了redux相关的内容，尝试去理解这个库的设计与使用。 其他随笔把旧blog迁移过来的事情还没有进度，放松的的时候一篇一篇来吧。","link":"/2023/05/10/book-0-read-about-js/"},{"title":"写给自己的博客使用笔记","text":"博客管理本博客为个人博客，联系方式见首页。 本文章记录一下博客的管理使用说明，防止每次都忘记。 添加文章1npx hexo new &quot;about ts type not in runtime&quot; # please run this command in project Folder 执行以上命令可以添加文章，然后就可以撰写文章了。 生成并发布网站1npx hexo g -d 执行该命令可以直接生成并重新发布网站。（提醒一下，记得关注网络状态）。 修改文章之后本地预览网站12npx hexo g #生成网站npx hexo s #生成预览服务器 官方教程如下Create a new post1hexo new &quot;My New Post&quot; More info: Writing Run server1hexo server More info: Server Generate static files1hexo generate More info: Generating Deploy to remote sites2023-11-9更新：使用了github的workflow自动部署了，这样就不用这样的方式了，git提交之后，就可以自动更新。 1hexo deploy More info: Deployment","link":"/2020/04/24/hello-world/"},{"title":"心境转变了一次记录","text":"心境变化的起因虽然很多时候，有“差生文具多”的嫌疑，我自己的google blog其实也没有太多问题（只是访问不了而已）。不过还是花了不到一个小时的时间把这个博客架设起来了～ 起因非常突然，今天突然看到了我的github绿色马赛克多了（如上图），我意识到自己最近一段时间状态不错😌。想着把博客也转过来吧，把笔记本里沉睡的一些东西写出来。虽然有点担心自己写的东西不成熟，也许日后看来是胡言乱语，但是想着还是公开出来比较好。未来可以整理吗～～ 我沉迷typescript了最近，开始痴迷了typescript和对对应的前端开发，react,next.js,vscode，github，等等很多东西让我有点静下心来了。有点点收获，比如，发现在ts中，reduce真的可以完全代替循环存在，于是乎，写出了以下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142const FitInfo = (carList: CarItem[]) =&gt; { const combine = (years: string[]): string =&gt; { const year_combine = years.reduce((acc, year) =&gt; { if (acc === '') return year const left = acc.slice(0, acc.lastIndexOf(',') + 1 + 4) const right = acc .slice(acc.lastIndexOf(',') + 1, acc.length) .slice(-4) if (Number(year) - Number(right) === 1) return left + '-' + year if (Number(year) - Number(right) &gt; 1) return acc + ',' + year return acc }, '') return year_combine } const description = sort(carList) .asc([(u) =&gt; u.make_name, (u) =&gt; u.model_name, (u) =&gt; u.year_name]) .map((v) =&gt; { return { carname: v.make_name + ' ' + v.model_name, year: v.year_name, } }) .reduce((acc, item) =&gt; { if ( !acc.some((it): boolean =&gt; { return it.carname === item.carname }) ) { acc.push({ carname: item.carname, year: [item.year] }) } else { const obj = acc.find((it) =&gt; it.carname === item.carname) obj?.year.push(item.year) } return acc }, [] as { carname: string; year: string[] }[]) .reduce((acc, item) =&gt; { return acc === '' ? item.carname + ' ' + combine(item.year) : acc + ';' + item.carname + ' ' + combine(item.year) }, '') return description} 我自己突然有些联想，长辈提过他上学的时候就是学习的lisp，不知道能否和他交流（大概率是不行吧🙍‍♂️）。lisp把数据和程序混在一起。我再看这段代码，循环完全是用reduce实现的，也许真的存在严禁证明for可以是不必要的。map、reduce就可以完成遍历操作。在我的程序中，为了实现目标，我引入了很多中间神奇的数据结构（指的是{ carname: string; year: string[] }），也许这就是数据和程序混在一起的感觉？利用数据结构来实现循环目标？不清楚了，希望我也能有朝一日接触到学院派的内容。当然，希望接触目标的时候能有足够的实践：比如，写个智能合约😊。 大概就是这么多，有点点困了，指针过了12点了，晚安。","link":"/2022/09/23/inner-peace/"},{"title":"某天在咖啡厅","text":"在一个咖啡厅重启了博客感觉每次写下博客总是在“重启”……都是更换平台，或者间隔好久，希望能把这次更换平台算到最后一次。 AI学习开始了一年前，我在kaggle上学习课程，同样是一年前，我开始学习react，如果回顾一下过去，我发现自己依然是“乱冲乱撞”。不过，确实收获了很多内容，这些学到的东西也许哪天能发挥作用。比如，了解了ts，发现了几乎是我梦寐以求的编程语言，顺路也了解了函数式编程。 另一个好消息是，2023年年初，AI应用爆发了，也许是个新的机会，如果我能抓住或者至少能利用起来就好了。依然希望我的未来能有更好的故事。 家里的好消息Rain考上大学了，真的是非常好的消息，全家人都开心。终于体会到那种我还把她当小姑娘但是其实已经是大孩子的感觉了。再有4年，小Rain也毕业了，那会儿会怎么样呢？3月的最后一天知道这个消息的，就把这天当作纪念日吧。 生活中充满了小惊喜生活中的小惊喜总是不断出现，比如，去买个蛋糕碰到了过家家的联名。 虽然，有时下雨不过总有开心的事情，对了，在妻子的帮助和鼓励下，我开始准备发展一些副业，也许这是新的难以想象的神奇经历的的开始。 学校的面条意外的好吃，吃了好多天，我还开玩笑的封这个面条为“天下第一面”，想起来去年的时候，想吃都吃不到。如果要说什么是小确幸的话，这个面条当之无愧吧。 找到了好玩的游戏买了《密特罗德究极重制版》，真的想把这种感觉记录下来。 游戏中的氛围非常有沉浸感，不断的升级、跑路、探索，果然是经典，能相遇太好了。目前游戏还没通关，准备单开一篇聊聊感受。","link":"/2023/04/02/some-day-in-the-coffee-shop/"},{"title":"start my AI side project with cf","text":"准备开始我的AI项目了读了两本书，感觉找回了当初“开卷有益”的感觉。从图书馆借书其实非常的方便，这让我想起了大学的时候，那会儿就是每天每天不停的看书。很多学到的东西收益终身。 现在要开始新的征程了，东撞西碰的回顾这么多年，发现github是个非常好的网站，即使几度放弃，结果，最终发现，最可控也最方便记录的居然是这个小小的静态博客。不知道有没有精力能把google.blog上的文章搬过来，也许这个目录未来会很大，不过，也许我也写不了那么多内容吧，谁知道呢。借助cloudflare的cdn和pages换了一个独立域名,发现意外的顺畅……再次感叹如果有好的IT基础生态，喜欢“折腾geek”得是每天都生活在幸福中吧。 在经过多次尝试和思考后，最后想着，要不先选择cloudflare这个平台（腾讯云上已经开通的虚拟机还是准备维持着），尝试开始新的项目，担忧也不是没有，只是希望这次能是个有反馈的长赛道。 后续的文章还是编号吧，这样便于管理，看了书，下一步就是熟悉各类项目了，先从记事本开始也许是个不错的主意。","link":"/2023/04/12/start-my-AI-side-project-with-cf/"}],"tags":[{"name":"生活","slug":"生活","link":"/tags/%E7%94%9F%E6%B4%BB/"},{"name":"教程","slug":"教程","link":"/tags/%E6%95%99%E7%A8%8B/"},{"name":"旧博客","slug":"旧博客","link":"/tags/%E6%97%A7%E5%8D%9A%E5%AE%A2/"},{"name":"读书","slug":"读书","link":"/tags/%E8%AF%BB%E4%B9%A6/"},{"name":"说明","slug":"说明","link":"/tags/%E8%AF%B4%E6%98%8E/"}],"categories":[],"pages":[]}